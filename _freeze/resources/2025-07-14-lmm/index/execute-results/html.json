{
  "hash": "5ad7055ad686f02059abc4edbd0de1cc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear mixed models\"\nauthor:  \"Emi Tanaka\"\ndate: 2025/07/14\ninstitute: Australian National University\nknitr: true\nexecute: \n  echo: true\nformat: \n  anu-light-revealjs:\n    width: 1920\n    height: 1080\n    disable-layout: false\n    auto-stretch: false\n    html-math-method: katex\n    css: [/assets/tachyons-addon.css]\n---\n\n\n\n\n## Linear mixed model\n\nA general form for the linear mixed model is given by \n\n\n\n::: {.cell}\n<style type=\"text/css\">\n.error {\n  color: rgb(252, 42, 141);\n}\n.random {\n  color: rgb(7, 149, 80);\n}\n.fixede {\n  color: rgb(8, 101, 181);\n}\n\n.highlight {\n  background-color: #F0EDCC;\n  padding: 7px;\n  color: #02343F;\n  font-weight: bold;\n  transform: skew(-7deg);\n  display: inline-block;\n}\n\n.box {\n  background-color: #F0EDCC;\n  color: #02343F;\n  padding: 20px;\n  border-radius: 20px;\n  margin-bottom: 7px;\n}\n.box p:first-child {\n  margin: 0!important;\n  display: block;\n  margin-bottom: 20px!important;\n}\n\n.box p:first-child > strong {\n  background-color: #02343F;\n  color: #F0EDCC;\n  padding: 20px;\n}\n\n</style>\n:::\n\n\n\n\n<center>\n\n![](images/lmm-processed.png)\n\n</center>\n\n- We let $\\boldsymbol{\\sigma} = (\\boldsymbol{\\sigma}^\\top_g, \\boldsymbol{\\sigma}^\\top_r)^\\top$ denote the complete vectors of variance parameters. \n- We need to estimate the [fixed effects $\\boldsymbol{\\beta}$]{.fixede} and [variance parameters $\\boldsymbol{\\sigma}$]{.fixede}.\n\n\n## Variance structures of [random effects]{.random}\n\n- The $q\\times 1$ vector of random effects is often composed of ${n_b}$ sub-vectors [$$\\boldsymbol{b} = (\\boldsymbol{b}_1^\\top,\\boldsymbol{b}_2^\\top,\\dots,\\boldsymbol{b}_{n_b}^\\top)^\\top$$]{.random} where the sub-vectors $\\boldsymbol{b}_i$ are of length $q_i$ and $\\sum_{i=1}^{n_b}q_i = q$.\n- Let $\\text{var}(\\boldsymbol{b}_i) = \\mathbf{G}_i$ and assuming $\\boldsymbol{b}_i$ are mutually independent, then\n\n$$\\mathbf{G} = \\oplus^{n_b}_{i=1} \\mathbf{G}_i = \\begin{bmatrix} \\mathbf{G}_1 & \\mathbf{0} & \\cdots & \\mathbf{0} \\\\ \\mathbf{0} & \\mathbf{G}_2 & \\cdots & \\mathbf{0} \\\\ \\vdots & \\vdots  & \\ddots & \\vdots \\\\ \\mathbf{0} & \\mathbf{0} &\\cdots & \\mathbf{G}_{n_b} \\\\ \\end{bmatrix} .$$\n\n- $\\mathbf{Z} =  \\begin{bmatrix} \\mathbf{Z}_1 & \\mathbf{Z}_2 & ... & \\mathbf{Z}_{n_b} \\\\ \\end{bmatrix}$ and $\\mathbf{Z}\\boldsymbol{b} = \\mathbf{Z}_1\\boldsymbol{b}_1 + \\cdots + \\mathbf{Z}_{n_b}\\boldsymbol{b}_{n_b} = \\sum_{i=1}^{n_b}\\mathbf{Z}_i\\boldsymbol{b}_i.$\n\n\n## Variance structures of [errors]{.error} \n\n- Classical assumption is that the errors are assumed to be independent and identically distributed (i.i.d)  $\\rightarrow\\mathbf{R} = \\sigma^2 \\mathbf{I}_n$.\n- In some situations, $\\boldsymbol{e}$ will be sub-divided into [_sections_]{.error}, e.g. multi-clinic trials or multi-environment variety trials.\n- We let [$\\boldsymbol{e} = (\\boldsymbol{e}_1^\\top, \\boldsymbol{e}_2^\\top, \\dots, \\boldsymbol{e}_{n_e}^\\top)^\\top$]{.error} so that $\\boldsymbol{e}_j$ represents the vector of errors of the $j$th section of the data.\n\n<center>\n\n$$\\mathbf{R} = \\oplus_{j=1}^{n_e} \\mathbf{R}_j = \n\\begin{bmatrix} \n\\mathbf{R}_1 & \\mathbf{0} & \\dots  &  \\mathbf{0} \\\\\n\\mathbf{0} & \\mathbf{R}_2 & \\dots  &  \\mathbf{0} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{0} & \\mathbf{0} & \\dots & \\mathbf{R}_{n_e}\n\\end{bmatrix}$$\n      \nwhere $\\oplus$ is the direct sum operator. \n\n</center>\n\n## Variance models\n\n- There are three types of variance model for $\\mathbf{R}_j$ and $\\mathbf{G}_i$:  \n\n<br>\n\n::: {.columns .f2}\n\n::: {.column width=\"33%\"}\n\n::: box\n\n **Correlation**\n\n - diagonal elements are all 1\n - off-diagonal elements are between -1 and 1  \n - If $\\mathbf{C}$ is a $n\\times n$ correlation model, then there are $\\frac{1}{2}n(n - 1)$ parameters. \n \n:::\n\n:::\n \n::: {.column width=\"33%\"}\n\n::: box\n\n**Homogeneous**\n\n- diagonal elements all have the same\npositive value $\\sigma^2$ (say)\n- $\\sigma^2\\mathbf{C}$ is a homogeneous model, so it has $\\frac{1}{2}n(n - 1) + 1$ parameters. \n\n:::\n\n:::\n\n::: {.column width=\"33%\"}\n\n::: box\n\n**Heterogeneous**\n\n- diagonal elements are positive but can differ $(\\sigma_1^2, \\sigma^2_2, \\cdots, \\sigma^2_n)$\n- $\\mathbf{D}\\mathbf{C}\\mathbf{D}$ where $\\mathbf{D} = \\oplus_{i=1}^n \\sigma_i$ is a heteregenous model, so it has $\\frac{1}{2}n(n - 1) + n$ parameters. \n\n:::\n\n:::\n\n:::\n\n\n\n \n\n## Variance model functions in `asreml`\n\n::: {.columns .f2}\n\n::: {.column width=\"25%\"}\n\n::: box\n\n**Default**\n\n`id()` $= \\mathbf{I}_n$\n\n`idv()` $= \\sigma^2\\mathbf{I}_n$\n\n`idh()` \n\n$$= \\begin{bmatrix}\\sigma^2_1 & 0 & \\cdots & 0 \\\\ \n0 & \\sigma^2_2 & \\ddots & \\vdots\\\\\n\\vdots & \\ddots  & \\ddots & 0 \\\\\n0 & \\cdots & 0 & \\sigma^2_n\n\\end{bmatrix}$$\n\n:::\n\n:::\n\n::: {.column width=\"25%\"}\n\n::: box\n\n**Time series**\n\n<br>\n\n\n`ar1()`, `ar2()`, `ar3()`, `sar()`, `sar2()`, `ma1()`, `ma3()`, `arma()`\n\n\n- Suffix name with `v` or `h` to convert correlation model to homogeneous or heterogeneous models.\n- E.g. `ar1v()` or `ar1h()`\n\n:::\n\n:::\n\n::: {.column width=\"25%\"}\n\n::: box\n\n**Metric-based**\n\n<br>\n\n\n`exp()`, `gau()`, `lvr()`, `iexp()`, `igau()`, `ieuc()`, `sph()`, `cir()`, `aexp()`, `agau()`, `mtrn()`\n\n- Suffix name with `v` or `h` to convert correlation model to homogeneous or heterogeneous models.\n\n:::\n\n:::\n\n::: {.column width=\"25%\"}\n\n::: box\n\n**General structures**\n\n<br>\n\n`cor()`, `corb()`, `corg()`, `diag()`, `us()`, `chol()`, `cholc()`, `ante()`, `sfa()`, `fa()`, `facv()`, `rr()`\n\n- For `cor()`, `corb()`, and `corg()`, suffix name with `v` or `h` to convert correlation model to homogeneous or heterogeneous models.\n\n:::\n\n:::\n\n:::\n\n- Some combination are equivalent, e.g. `idh()` $\\equiv$ `diag()` and `corgh()` $\\equiv$ `us()`, but default starting values or computation under the hood may differ.\n\n\n\n## Aliasing of variance parameters\n\n\n::: incremental\n \n- Variance models may not be:\n  - identifiable if they are over-parameterised,\n  - insufficient data to estimate the parameters of the chosen variance\n model.\n- Note that if you have $\\mathbf{V}_1 \\otimes \\mathbf{V}_2$, where $\\mathbf{V}_1$ and $\\mathbf{V}_2$ are variance models, then this is over-parameterised. E.g. `idv(col):idv(row)`.\n- $\\mathbf{C} \\otimes \\mathbf{V}$ or $\\mathbf{V} \\otimes \\mathbf{C}$, where $\\mathbf{V}$ is a variance model and $\\mathbf{C}$ is a correlation model, may be acceptable. This is referred to as the **_sigma parameterization_** in `asreml`. E.g. `idv(col):id(row)`. \n- $\\mathbf{C} \\otimes \\mathbf{C}$ is generally converted to $\\sigma^2 \\mathbf{C} \\otimes \\mathbf{C}$ by `asreml`. This is referred to as the **_gamma parameterization_** in `asreml`. E.g. `id(col):id(row)`. \n\n:::\n\n\n## Example ðŸŒ¾ different variance models\n\n::: f3\n\n\n\n::: {.columns }\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"5,11,17\"}\nlibrary(asreml)\n# gamma parameterization.\nfiti <- asreml(yield ~ 1,\n               random =~ Variety,\n               residual =~ id(column):id(row),\n               data = naf)\n\n# sigma parameterization\nfitv <- asreml(yield ~ 1,\n               random =~ Variety,\n               residual =~ idv(column):id(row),\n               data = naf)\n\n# sigma parameterization\nfith <- asreml(yield ~ 1,\n               random =~ Variety,\n               residual =~ idh(column):id(row),\n               data = naf)\n```\n:::\n\n\n\nBelow cannot be fitted. \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"3\"}\nfitn <- asreml(yield ~ 1,\n               random =~ Variety,\n               residual =~ idv(column):idv(row),\n               data = naf)\n```\n:::\n\n\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom.asreml)\n\ntidy(fiti, \"varcomp\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 5\n  term         estimate std.error statistic constraint\n  <chr>           <dbl>     <dbl>     <dbl> <chr>     \n1 Variety        0.0860    0.0303      2.84 P         \n2 column:row!R   0.102     0.0220      4.63 P         \n```\n\n\n:::\n\n```{.r .cell-code}\ntidy(fitv, \"varcomp\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 5\n  term              estimate std.error statistic constraint\n  <chr>                <dbl>     <dbl>     <dbl> <chr>     \n1 Variety             0.0860    0.0303      2.84 P         \n2 column:row!column   0.102     0.0220      4.63 P         \n3 column:row!R        1        NA          NA    F         \n```\n\n\n:::\n\n```{.r .cell-code}\ntidy(fith, \"varcomp\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 14 Ã— 5\n   term                 estimate std.error statistic constraint\n   <chr>                   <dbl>     <dbl>     <dbl> <chr>     \n 1 Variety                0.0543   0.0181       3.00 P         \n 2 column:row!R           1       NA           NA    F         \n 3 column:row!column_1    0.0225   0.0199       1.13 P         \n 4 column:row!column_2    0.0153   0.00982      1.56 P         \n 5 column:row!column_3    0.0267   0.0201       1.32 P         \n 6 column:row!column_4    0.146    0.0738       1.97 P         \n 7 column:row!column_5    0.226    0.100        2.25 P         \n 8 column:row!column_6    0.0240   0.0179       1.34 P         \n 9 column:row!column_7    0.326    0.142        2.30 P         \n10 column:row!column_8    0.0939   0.0503       1.87 P         \n11 column:row!column_9    0.220    0.102        2.15 P         \n12 column:row!column_10   0.0788   0.0435       1.81 P         \n13 column:row!column_11   0.116    0.0639       1.82 P         \n14 column:row!column_12   0.292    0.131        2.23 P         \n```\n\n\n:::\n:::\n\n\n\n:::\n:::\n\n\n\n:::\n\n\n\n\n## BLUEs and BLUPs\n\n- Suppose that $\\text{var}(\\boldsymbol{y}) = \\mathbf{V} = \\mathbf{Z}\\mathbf{G}\\mathbf{Z}^\\top + \\mathbf{R}$.\n- Also assume $\\boldsymbol{\\sigma}$ is known, $\\mathbf{V}$ is non-singular and $\\mathbf{X}$ is full rank. \n- Let $\\mathbf{P} = \\mathbf{V}^{-1} - \\mathbf{V}^{-1}\\mathbf{X}(\\mathbf{X}^\\top\\mathbf{V}^{-1}\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{V}^{-1}$\n\n\n::: columns\n\n::: {.column width = \"50%\"}\n\n::: box\n\n**Best linear unbiased estimates (BLUEs)**\n\n$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top\\mathbf{V}^{-1}\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{V}^{-1}\\boldsymbol{y}$$\n\n:::\n\n:::\n\n::: {.column width = \"50%\"}\n\n\n::: box\n\n**Best linear unbiased predictions (BLUPs)**\n\n$$\\tilde{\\boldsymbol{b}} = \\mathbf{G}\\mathbf{Z}^\\top\\mathbf{P}\\boldsymbol{y} = \\mathbf{G}\\mathbf{Z}^\\top\\mathbf{V}^{-1}(\\boldsymbol{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})$$\n\n\n\n:::\n\n\n:::\n\n:::\n\n\n- But since $\\mathbf{V}$ is often unknown, it is estimated from the data. \n- Correspondingly, we plug the estimate of $\\mathbf{V}$ to above to get the E-BLUEs and E-BLUPs (where E = empirical).\n\n::: aside\n\nFirst usage of the term BLUP in Goldberger (1962) Best Linear Unbiased Prediction in the Generalized Linear Regression Model. _Journal of American Statistical Association_\n\n:::\n\n\n## Example ðŸŒ¾ BLUEs and BLUPs by hand\n\n::: {.columns .f3}\n::: {.column width=\"40%\"}\n\nBy `asreml`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfits  <- asreml(yield ~ 1,\n               random =~ Variety,\n               residual =~ units,\n               data = shf)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(fits, \"fixed\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 3\n  term        estimate std.error\n  <chr>          <dbl>     <dbl>\n1 (Intercept)    1470.    0.0161\n```\n\n\n:::\n\n```{.r .cell-code}\ntidy(fits, \"random\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 25 Ã— 3\n   term       estimate std.error\n   <chr>         <dbl>     <dbl>\n 1 Variety_1   -156.       0.103\n 2 Variety_2     20.8      0.103\n 3 Variety_3     -7.78     0.103\n 4 Variety_4    -26.3      0.103\n 5 Variety_5    -18.2      0.103\n 6 Variety_6      4.43     0.103\n 7 Variety_7    -42.2      0.103\n 8 Variety_8     45.6      0.103\n 9 Variety_9    -92.2      0.103\n10 Variety_10  -161.       0.103\n# â„¹ 15 more rows\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n::: {.column width=\"60%\" .fragment}\n\nBy \"hand\"\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvars <- tidy(fits, \"varcomp\")$estimate\nG <- vars[1] * diag(nlevels(shf$Variety))\nR <- vars[2] * diag(nrow(shf))\nZ <- model.matrix(~Variety - 1, data = shf)\nX <- matrix(1, nrow = nrow(shf))\nV <- Z %*% G %*% t(Z) + R\ny <- shf$yield\n(BLUE <- solve(t(X) %*% solve(V) %*% X) %*% t(X) %*% solve(V) %*% y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        [,1]\n[1,] 1470.44\n```\n\n\n:::\n\n```{.r .cell-code}\n(BLUP <- G %*% t(Z) %*% solve(V) %*% (y - X %*% BLUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             [,1]\n [1,] -156.452173\n [2,]   20.841535\n [3,]   -7.779433\n [4,]  -26.339105\n [5,]  -18.231459\n [6,]    4.430877\n [7,]  -42.163667\n [8,]   45.652885\n [9,]  -92.177099\n[10,] -161.238614\n[11,]  -66.388923\n[12,]    7.849764\n[13,]   92.247430\n[14,]  -54.569343\n[15,]  -14.714889\n[16,]   -6.997973\n[17,]   22.892867\n[18,]   87.167941\n[19,]  101.038854\n[20,]  144.116829\n[21,]  -27.804342\n[22,]  106.118343\n[23,]  -78.696916\n[24,]   43.113141\n[25,]   78.083470\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n:::\n\n\n\n## Best linear unbiased estimator\n\n- A statistic $\\hat{\\boldsymbol{\\beta}}$ is the [best linear unbiased estimator]{.fixede} of $\\boldsymbol{\\beta}$ if \n  - $\\hat{\\boldsymbol{\\beta}} =  \\mathbf{A}\\boldsymbol{y}$ for some $\\mathbf{A}$,  \n  - $E(\\hat{\\boldsymbol{\\beta}}) = \\boldsymbol{\\beta}$, and\n  - $\\text{var}(\\hat{\\boldsymbol{\\beta}})$ is the smallest of all linear unbiased estimator of $\\boldsymbol{\\beta}$.\n  \n  \n- Homework: prove $\\boldsymbol{c}_1^\\top\\hat{\\boldsymbol{\\beta}} + \\boldsymbol{c}_2^\\top\\tilde{\\boldsymbol{b}}$ is BLUE for $\\boldsymbol{c}_1^\\top\\boldsymbol{\\beta} + \\boldsymbol{c}_2^\\top\\boldsymbol{b}$ for any $\\boldsymbol{c}_1 \\in\\mathbb{R}^p$ and $\\boldsymbol{c}_2 \\in\\mathbb{R}^q$.\n\n\n\n\n## Objective function to derive BLUEs and BLUPs\n\n- Find $(\\boldsymbol{\\beta}, \\boldsymbol{b})$ which maximises the [log-density function of the joint distribution of $\\boldsymbol{y}$ and $\\boldsymbol{b}$]{.fixede} assuming $\\boldsymbol{\\sigma}$ is known.\n- Note $\\boldsymbol{y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\mathbf{V})$, $\\boldsymbol{b} \\sim N(\\boldsymbol{0}, \\mathbf{G})$ and $\\boldsymbol{y}|\\boldsymbol{b} \\sim N(\\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{b}, \\mathbf{R})$.\n\n\n\\begin{align*}\n\\ell &= \\log f(\\boldsymbol{y}, \\boldsymbol{b} ; \\boldsymbol{\\sigma})\\\\\n&= \\log f(\\boldsymbol{y}|\\boldsymbol{b};\\boldsymbol{\\sigma}_r) + \\log f(\\boldsymbol{b};\\boldsymbol{\\sigma}_g) \\\\\n&= -\\frac{1}{2}\\log |\\mathbf{R}| - \\frac{1}{2}(\\boldsymbol{y}-\\mathbf{X}\\boldsymbol{\\beta}-\\mathbf{Z}\\boldsymbol{b})^\\top \\mathbf{R}^{-1}(\\boldsymbol{y}-\\mathbf{X}\\boldsymbol{\\beta}-\\mathbf{Z}\\boldsymbol{b}) \\\\\n& \\qquad - \\frac{1}{2}\\log|\\mathbf{G}|  - \\frac{1}{2} \\boldsymbol{b}^\\top\\mathbf{G}^{-1}\\boldsymbol{b} - \\frac{n+q}{2}\\log 2\\pi.\n\\end{align*}\n\n## Mixed model equations \n\n\\begin{align*}\n\\frac{\\partial \\ell}{\\partial\\boldsymbol{\\beta}} &= \\mathbf{X}^\\top \\mathbf{R}^{-1}(\\boldsymbol{y} - \\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{Z}\\boldsymbol{b}) = \\boldsymbol{0} \\\\\n\\frac{\\partial \\ell}{\\partial\\boldsymbol{b}} &= \\mathbf{Z}^\\top \\mathbf{R}^{-1}(\\boldsymbol{y} - \\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{Z}\\boldsymbol{b}) - \\mathbf{G}^{-1} \\boldsymbol{u}= \\boldsymbol{0}. \n\\end{align*}\n\nRearranging the above results in the [**mixed model equations**]{.fixede}:\n\n$$\\underbrace{\\begin{bmatrix}\n\\mathbf{X}^\\top \\mathbf{R}^{-1}\\mathbf{X} & \\mathbf{X}^\\top \\mathbf{R}^{-1}\\mathbf{Z} \\\\\n\\mathbf{Z}^\\top \\mathbf{R}^{-1}\\mathbf{X} & \\mathbf{Z}^\\top \\mathbf{R}^{-1}\\mathbf{Z} + \\mathbf{G}^{-1} \\\\\n\\end{bmatrix}}_{\\normalsize\\mathbf{C}}\n\\begin{bmatrix}\n\\hat{\\boldsymbol{\\beta}} \\\\\n\\tilde{\\boldsymbol{b}}\n\\end{bmatrix}\n= \n\\begin{bmatrix}\n\\mathbf{X}^\\top\\mathbf{R}^{-1}\\boldsymbol{y}\\\\\n\\mathbf{Z}^\\top\\mathbf{R}^{-1}\\boldsymbol{y}\n\\end{bmatrix}$$\n\nSolving this results in the same BLUE and BLUP as before.\n\n::: aside\n\nHenderson (1949) Estimation of changes in herd environment. (Abstract) _Journal of Dairy Science_  \nHenderson (1950) Estimation of genetic parameters. _Annals of Mathematical Statistics_\n\n\n\n:::\n\n\n\n\n## Prediction error variance {.scrollable}\n\n::: box\n\n**Variance of prediction errors**\n\n$$\\text{var}\\left(\\begin{bmatrix}\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}\\\\\\tilde{\\boldsymbol{b}} - \\boldsymbol{b}\\end{bmatrix}\\right) = \\mathbf{C}^{-1} = {\\scriptsize\\begin{bmatrix}(\\mathbf{X}^\\top\\mathbf{V}^{-1}\\mathbf{X})^{-1} & -(\\mathbf{X}^\\top\\mathbf{V}^{-1}\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{V}^{-1}\\mathbf{Z}\\mathbf{G}\\\\ -\\mathbf{G}\\mathbf{Z}^\\top\\mathbf{V}^{-1}\\mathbf{X}(\\mathbf{X}^\\top\\mathbf{V}^{-1}\\mathbf{X})^{-1} & \\mathbf{G} - \\mathbf{G}\\mathbf{Z}^\\top\\mathbf{P}\\mathbf{Z}\\mathbf{G} \\end{bmatrix}}$$\n\n:::\n\n\n- Note $\\text{var}(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}) = \\text{var}(\\hat{\\boldsymbol{\\beta}})$.\n\n<br>\n\n::: {.columns .f3}\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitc <- asreml(yield ~ 1,\n               random =~ Variety,\n               residual =~ units,\n               data = shf,\n               options = asreml.options(\n                 Cfixed = TRUE,\n                 Csparse =~ Variety\n               ))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfitc$Cfixed\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 707.637\n```\n\n\n:::\n\n```{.r .cell-code}\nsp2mat(fitc$Csparse)[1:5, 1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 4534.929    0.000    0.000    0.000    0.000\n[2,]    0.000 4534.929    0.000    0.000    0.000\n[3,]    0.000    0.000 4534.929    0.000    0.000\n[4,]    0.000    0.000    0.000 4534.929    0.000\n[5,]    0.000    0.000    0.000    0.000 4534.929\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\nBy \"hand\"\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsolve(t(X) %*% solve(V) %*% X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]\n[1,] 707.8299\n```\n\n\n:::\n\n```{.r .cell-code}\nP <- solve(V) - solve(V) %*% X %*% solve(t(X) %*% solve(V) %*% X) %*% t(X) %*% solve(V)\nCZZ <- G - G %*% t(Z) %*% P %*% Z %*% G\nCZZ[1:5, 1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          [,1]      [,2]      [,3]      [,4]      [,5]\n[1,] 4535.9150  243.1447  243.1447  243.1447  243.1447\n[2,]  243.1447 4535.9150  243.1447  243.1447  243.1447\n[3,]  243.1447  243.1447 4535.9150  243.1447  243.1447\n[4,]  243.1447  243.1447  243.1447 4535.9150  243.1447\n[5,]  243.1447  243.1447  243.1447  243.1447 4535.9150\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n## Estimation of variance parameters\n\n- Variance parameters $\\boldsymbol{\\sigma}$ can be estimated using **maximum likelihood** (ML) approach or **residual (or restricted) maximum likelihood** (REML) approach.\n- REML estimates takes into account the loss of degrees of freedom associated with estimation of fixed effects, so are _less biased_ than ML estimates. \n\n::: {.columns .f3}\n::: {.column width=\"40%\"}\n\n::: box\n\n**ML**\n\n\\begin{align*}\n\\hat{\\boldsymbol{\\sigma}}_{\\text{ML}} &= \\operatorname*{arg\\,max}_{\\boldsymbol{\\sigma}} f(\\boldsymbol{y}; \\boldsymbol{\\sigma})\\\\\n&= \\operatorname*{arg\\,max}_{\\boldsymbol{\\sigma}}~ \\log f(\\boldsymbol{y}; \\boldsymbol{\\sigma})\\\\\n&= \\operatorname*{arg\\,max}_{\\boldsymbol{\\sigma}}~  -\\frac{1}{2}\\left( \\log |\\mathbf{V}| + (\\boldsymbol{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^\\top\\mathbf{V}^{-1}(\\boldsymbol{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})\\right)\n\\end{align*}\n\n\n:::\n\n:::\n\n::: {.column width=\"60%\"}\n\n\n::: box\n\n**REML**\n\n\n\n\\begin{align*}\n\\hat{\\boldsymbol{\\sigma}}_{\\text{REML}} &= \\operatorname*{arg\\,max}_{\\boldsymbol{\\sigma}} \\ell_R(\\boldsymbol{\\sigma}) \\quad(\\textbf{residual log-likelihood})\\\\\n&= \\operatorname*{arg\\,max}_{\\boldsymbol{\\sigma}}~  -\\frac{1}{2}\\left( \\log |\\mathbf{V}| + {\\color{RoyalBlue} \\log|\\mathbf{X}^\\top\\mathbf{V}^{-1}\\mathbf{X}|}   + (\\boldsymbol{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^\\top\\mathbf{V}^{-1}(\\boldsymbol{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})\\right)\\\\\n&= \\operatorname*{arg\\,max}_{\\boldsymbol{\\sigma}}~ -\\frac{1}{2}\\left(\\log |\\mathbf{G}| +  \\log |\\mathbf{R}| + \\log |\\mathbf{C}| + \\boldsymbol{y}^\\top\\mathbf{P}\\boldsymbol{y}\\right)\n\\end{align*}\n\n\n:::\n\n:::\n:::\n\n\n\n\n::: aside \n\nHartley & Rao (1967) Maximum-likelihood estimation for the mixed analysis of variance model. _Biometrika_\n\nPatterson & Thompson (1971) Recovery of inter-block information when block sizes are unequal. _Biometrika_\n\n\nVerbyla (1990) A conditional derivation of residual maximum likelihood. _Australian & New Zealand Journal of Statistics_\n\n\nSmith (1999) Multiplicative mixed models for the analysis of multi-environment trial data. _PhD Thesis_\n\n:::\n\n\n## REML score equations {.f2}\n\n::: {.box}\n\n**REML score equations**\n\nREML estimates for $\\boldsymbol{\\sigma}_{m\\times1} = (\\sigma_1, \\dots, \\sigma_m)^\\top$ are obtained by solving  $$s_i(\\boldsymbol{\\sigma}) = \\frac{\\partial\\ell_R(\\boldsymbol{\\sigma})}{\\partial  \\sigma_i} = 0 \\quad\\text{for } i = 1, \\dots, m.$$\n\n:::\n\n- We solve numerically using a Newton-Raphson algorithm: $$\\boldsymbol{\\sigma}^{(t + 1)} = \\boldsymbol{\\sigma}^{(t)} + \\left({\\color{RoyalBlue}\\mathcal{J}(\\boldsymbol{\\sigma}^{(t)}})\\right)^{-1}\\boldsymbol{s}(\\boldsymbol{\\sigma}^{(t)})$$ where \n\n  - $\\boldsymbol{s}(\\boldsymbol{\\sigma}^{(t)}) = (s_1(\\boldsymbol{\\sigma}^{(t)}), s_2(\\boldsymbol{\\sigma}^{(t)}), \\dots, s_m(\\boldsymbol{\\sigma}^{(t)}))^\\top$ is the $m\\times 1$ scores at iteration $t$, and\n  - $\\mathcal{J}(\\boldsymbol{\\sigma}^{(t)})$ is the $m\\times m$ [**observed information matrix**]{.fixede} at iteration $t$ where the $(i,j)$-th entry is given as $$\\mathcal{J}_{ij}(\\boldsymbol{\\sigma}^{(t)}) = -\\dfrac{\\partial s_i(\\boldsymbol{\\sigma}^{(t)})}{\\partial \\sigma_j} = -\\frac{\\partial^2\\ell_R(\\boldsymbol{\\sigma}^{(t)})}{\\delta\\sigma_i\\delta\\sigma_j}.$$\n  \n  \n## Information matrices\n\n::: f2\n\nLet $\\mathbf{V}'_i = \\frac{\\partial \\mathbf{V}}{\\partial \\sigma_i}$ and $\\mathbf{V}''_i = \\frac{\\partial^2 \\mathbf{V}}{\\partial \\sigma_i\\partial \\sigma_j}$.\n\n\n\n::: box\n\n**Observed information matrix**\n\n$$\\mathcal{J}_{ij} = \\frac{1}{2}\\text{tr}(\\mathbf{P}\\mathbf{V}''_{ij}) - \\frac{1}{2}\\text{tr}(\\mathbf{P}\\mathbf{V}_{i}'\\mathbf{P}\\mathbf{V}_j') + \\boldsymbol{y}\\mathbf{P}\\mathbf{V}'_i\\mathbf{P}\\mathbf{V}_j'\\mathbf{P}\\boldsymbol{y}-\\frac{1}{2}\\boldsymbol{y}^\\top\\mathbf{P}\\mathbf{V}''_{ij}\\mathbf{P}\\boldsymbol{y}$$\n\n:::\n\n::: box\n\n**Expected information matrix (Fisher information)**\n\n$$\\mathcal{I}_{ij} = E(\\mathcal{J}_{ij}) = \\frac{1}{2}\\text{tr}(\\mathbf{P}\\mathbf{V}_{i}'\\mathbf{P}\\mathbf{V}_j') $$\n\n:::\n\n\n::: box\n\n**Average information matrix**\n\n$$\\mathcal{A}_{ij} = \\frac{1}{2}\\boldsymbol{y}\\mathbf{P}\\mathbf{V}'_i\\mathbf{P}\\mathbf{V}_j'\\mathbf{P}\\boldsymbol{y} = \\frac{1}{2}(\\mathcal{J}_{ij} + \\mathcal{I}_{ij})\\quad\\text{assuming }\\boldsymbol{y}^\\top\\mathbf{P}\\mathbf{V}''_{ij}\\mathbf{P}\\boldsymbol{y} = \\text{tr}(\\mathbf{P}\\mathbf{V}''_{ij})$$\n\n:::\n\n:::\n\n\n::: aside \n\n\n::: f3\n\nJohnson & Thompson (1995) Restricted Maximum Likelihood Estimation of Variance Components for Univariate Animal Models Using Sparse Matrix Techniques and Average Information. _Journal of Dairy Science_\n\n\nGilmour, Thompson & Cullis (1995) Average Information REML: An Efficient Algorithm for Variance Parameter Estimation in Linear Mixed Models. _Biometrics_\n\n:::\n\n:::\n\n\n## Scores  {visibility=\"hidden\"}\n\n$$s_i = \\begin{cases}-\\frac{1}{2}\\left(\\text{tr}(\\mathbf{G}^{-1}_i\\mathbf{G}'_{ij} - \\text{tr}(\\mathbf{G}_i^{-1}\\mathbf{G}'_{ij}\\mathbf{G}_i^{-1}\\mathbf{C}^{Z_iZ_i}) - \\tilde{\\boldsymbol{b}}^\\top_i\\mathbf{G}_i^{-1}\\mathbf{G}'_{ij}\\mathbf{G}_i^{-1}\\tilde{\\boldsymbol{b}}_i)\\right)\\\\-\\frac{1}{2}\\left(\\text{tr}(\\mathbf{R}^{-1}\\mathbf{R}'_i) - \\text{tr}(\\mathbf{C}^{-1}\\mathbf{W}^\\top\\mathbf{R}^{-1}\\mathbf{R}'_i\\mathbf{R}^{-1}\\mathbf{W}) - \\tilde{\\boldsymbol{e}}^\\top\\mathbf{R}^{-1}\\mathbf{R}'_i\\mathbf{R}^{-1}\\tilde{\\boldsymbol{e}}\\right)\\end{cases}$$\n\n\n## Algorithm overview {style=\"font-size:0.9em;\"}\n\nStart with initial estimate of $\\boldsymbol{\\sigma} = \\boldsymbol{\\sigma}^{(0)}$.\n\nFor $t = 1, \\dots, \\texttt{maxit}$,\n\n1. Calculate $\\hat{\\boldsymbol{\\beta}}^{(t - 1)} = (\\mathbf{X}^\\top(\\mathbf{V}^{(t -1)})^{-1}\\mathbf{X})^{-1}\\mathbf{X}^\\top(\\mathbf{V}^{(t -1)})^{-1}\\boldsymbol{y}$ and $\\tilde{\\boldsymbol{b}}^{(t - 1)} = \\mathbf{G}\\mathbf{Z}^\\top\\mathbf{P}^{(t - 1)}\\boldsymbol{y}$.\n2. Calculate residual likelihood $\\ell_R^{(t- 1)} = -\\frac{1}{2}\\left(\\log |\\mathbf{G}^{(t-1)}| +  \\log |\\mathbf{R}^{(t-1)}| + \\log |\\mathbf{C}^{(t-1)}| + \\boldsymbol{y}^\\top\\mathbf{P}^{(t-1)}\\boldsymbol{y}\\right)$'\n3. Calculate the scores $\\boldsymbol{s}(\\boldsymbol{\\sigma}^{(t - 1)})$.\n4. Calculate the average informaton matrix $\\mathcal{A}_{ij}^{(t - 1)} = \\frac{1}{2}\\boldsymbol{y}\\mathbf{P}^{(t-1)}\\mathbf{V}'^{(t-1)}_i\\mathbf{P}^{(t-1)}\\mathbf{V}_j'^{(t-1)}\\mathbf{P}^{(t-1)}\\boldsymbol{y}$\n5. Update $\\boldsymbol{\\sigma}^{(t)} = \\boldsymbol{\\sigma}^{(t - 1)} + \\left(\\mathcal{A}^{(t - 1)}\\right)^{-1}\\boldsymbol{s}(\\boldsymbol{\\sigma}^{(t-1)})$\n6. Repeat 1-2, if $\\ell_R^{(t)} - \\ell_R^{(t-1)} < \\text{threshold}$ then break, otherwise continue from Step 3.\n\n  \n\n## References\n\n\n- Smith (1999) Multiplicative mixed models for the analysis of multi-environment trial data. PhD Thesis.\n- Searle (2006) Matrix Algebra Useful for Statistics. \n- Searle et al (2006) Variance Components.\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}